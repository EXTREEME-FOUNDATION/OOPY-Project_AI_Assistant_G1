{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# AI Assistant\n",
    "## Group Members:\n",
    " - Krylova Alena\n",
    " - Dudic Mateja\n",
    " - Saavedra Triana Erwin Omar\n",
    " - Maringer Kelvin\n",
    "\n",
    "## Python Versions used:\n",
    " - 3.13\n",
    " - 3.14\n",
    "\n",
    "## Contributions:\n",
    " - Krylova Alena:\n",
    "     - Dataset Overview\n",
    "     - Data Quality Check\n",
    "     - Additional Insights (4-5)\n",
    " - Dudic Mateja:\n",
    "     - Data Analysis (until (excl.) \"Visualizations\")\n",
    " - Saavedra Triana Erwin Omar:\n",
    "     - Data Preprocesing\n",
    "     - Aditional Insights (1-3)\n",
    " - Maringer Kelvin:\n",
    "     - Data Analysis (starting from (incl.) \"Visualizations\")"
   ],
   "id": "62561f7da35786b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# FIRST TIME SETUP\n",
    "# ----------------\n",
    "# MAKE SURE THAT THIS CELL RUNS WITHOUT ERRORS BEFORE PROCEEDING\n",
    "# ----------------"
   ],
   "id": "55dd7ead145094d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# All the packages that have to be INSTALLED should be listed here\n",
    "%pip install numpy pandas matplotlib seaborn kagglehub ipywidgets --quiet\n",
    "# This will filter out the output from Jupyter Notebooks when committing to git, so that diffs are cleaner\n",
    "! git config filter.strip-notebook-output.clean \"jupyter nbconvert --ClearOutputPreprocessor.enabled=True --to=notebook --stdin --stdout --log-level=ERROR\"\n",
    "\n",
    "import kagglehub\n",
    "import platform\n",
    "\n",
    "# Download latest version\n",
    "dataset_path = kagglehub.dataset_download(\"prince7489/daily-ai-assistant-usage-behavior-dataset\") + (\"/Daily_AI_Assistant_Usage_Behavior_Dataset.csv\" if platform.system() != \"Windows\" else \"\\\\Daily_AI_Assistant_Usage_Behavior_Dataset.csv\")\n",
    "\n",
    "print(\"Path to dataset files:\", dataset_path)"
   ],
   "id": "3515412d721f4d1a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ----------------------\n",
    "# ----------------------"
   ],
   "id": "93d9ce06d522dd74"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#All the IMPORTS should be listed here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plot\n",
    "import seaborn as sea\n"
   ],
   "id": "70d39d9ec2e77d19"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## *Dataset Overview*\n",
    "The Daily AI Assistant Usage Behavior Dataset captures real-world patterns of how users interact with AI assistants throughout their daily activities. It provides insights into when, how, and for what purposes people used AI tools, as well as session characteristics and user satisfaction.\n",
    "\n",
    "The dataset is published on the Kaggle platform and is intended for researchers, developers, and data science practitioners interested in user behavior analysis, personalization systems, recommendation engines, and conversational AI. It covers a wide range of AI usage scenarios, including learning, productivity, research, and routine daily tasks.\n",
    "\n",
    "The dataset contains 300 rows and 8 columns.\n",
    "\n",
    "*Features (their meaning and data types):*  \n",
    "1st column: timestamp - date and time when the interaction with an AI tool started, data type - categorical (string)  \n",
    "2nd column: device - type of device which was used to access an AI tool (desktop, mobile, smart speaker), data type - categorical (string)  \n",
    "3rd column: usage_category - for what purpose the user used an AI tool (education, daily tasks, research and etc), data type - categorical (string)  \n",
    "4th column: prompt_length - lenght of the user`s prompt (measured in charakters), data type - integer  \n",
    "5th column: session_length_minutes - duration of the session in minutes, data type - float  \n",
    "6th column: satisfaction_rating - user satisfaction score from 1 to 5, data type - integer  \n",
    "7th column: assistant_model - which AI assistant model was used during the session, data type - categorical(string)  \n",
    "8th column: tokens_used - number of tokens used during the session, data type - integer  \n",
    "\n",
    "Most features from the data set are categorical, making the dataset suitable for analyzing patterns and user behavior segmentation (for example, feature 'timestamp' allows to see if people use AI tools more often on weeekdays or weekends, in the mornings or in the evenings).\n",
    "\n",
    "To obtain a statistical summary of the numerical features, the describe() method was used.\n",
    "It provided key statistics such as mean, standard deviation, minimum and maximum values, as well as quartiles. It allows to better understand distributaion of data.   \n",
    "*Some observations from the desccribe() function:*   \n",
    "The average prompt length is 129 characters, it indicates that users often submit detailed prompts.  \n",
    "The average session duration is about 7.7 minutes, indicating that most interactions with the AI assistant are relatively short.  \n",
    "The average satisfaction rating is close to 3 (on a scale from 1 to 5), which shows users` experience in general is neutral (or positiv.)  \n",
    "Token usage varies significantly, showing the differences in query complexity.  \n"
   ],
   "id": "197474c45c9fd9fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## Code for the dataset overview Here\n",
    "data = pd.read_csv(dataset_path)\n",
    "print(data.head())\n",
    "print(data.info())   # to get information about the dataframe (number of rows, columns, data types)\n",
    "# to get basic statistics about the dataframe (mean, std, min, max, etc.)\n",
    "print(data.describe())"
   ],
   "id": "ef6635f7a83d9746"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## *Data Quality Check*\n",
    "\n"
   ],
   "id": "9089078be7e42925"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## Code for the data quality check Here\n",
    "print(f\"Missing values per columns: \\n {data.isnull().sum()}\") \n",
    "# The dataset contains no missing values\n",
    "rows = 1\n",
    "cols = len(data.select_dtypes(include=[np.number]).columns)\n",
    "fig, axes = plot.subplots(rows, cols, figsize=(15,5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(data.select_dtypes(include=[np.number]).columns):\n",
    "    sea.boxplot(y=data[col], ax=axes[i])\n",
    "plot.suptitle(\"Boxplot for the numerical columns \")\n",
    "plot.show()\n",
    "\n",
    "#from the boxplots we can see that there are no outliers in the numerical columns\n",
    "\n",
    "print(data['device'].value_counts())\n",
    "print(data['usage_category'].value_counts())\n",
    "print(data['assistant_model'].value_counts())\n",
    "# For the categorical columns, we examined  the unique values and their frequencies. There are no unusual or extremely rare entries, so no outliers were detected in the dataset.\n"
   ],
   "id": "3980c9561a5500ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## *Data-Preprocessing*\n",
    " - Additional Notes etc..."
   ],
   "id": "8f13b6b638dd77ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# From here you can take for the Data Quality Check\n",
    "RAW_data = pd.read_csv(dataset_path)\n",
    "# first we check the number of missing values in each column\n",
    "print(RAW_data.isnull().sum())\n",
    "# after checking we can see that there are no missing values in the dataset\n",
    "# so for the outliers in this dataset , I think the best aproach would be to leave them and just mark them as outliers. In this dataset the outliers might be relevant data from users that have a diferent behavior than the average user, so removing them would mean losing relevant data.\n",
    "# now we will check for outliers using the IQR method\n",
    "y = RAW_data.select_dtypes(include=[np.number])\n",
    "print(y)\n",
    "for column in y:\n",
    "    quartile_min = RAW_data[column].quantile(0.25)\n",
    "    quartile_max = RAW_data[column].quantile(0.75)\n",
    "\n",
    "    IQR = quartile_max - quartile_min\n",
    "\n",
    "    lower_bound = quartile_min - 1.5 * IQR\n",
    "    upper_bound = quartile_max + 1.5 * IQR\n",
    "\n",
    "    outliers_promt_length = RAW_data[(RAW_data[column] < lower_bound) | (RAW_data[column] > upper_bound)].count()\n",
    "    outliers_promt_length = outliers_promt_length.sum()\n",
    "\n",
    "    print(f\"the number of outliers in {column} is the following: \\n{outliers_promt_length}\")\n",
    " # Start of the data preprocessing\n",
    "\n",
    "# after checking we can see that there are no outliers in the dataset, surprinsing but good.\n",
    "# next we will check for duplicates in the dataset\n",
    "\n",
    "x = RAW_data.duplicated().sum()\n",
    "print(f\"the number of duplicates in the dataset is the followings: \\n{x}\")\n",
    "\n",
    "# after checking we can see that there are no duplicates in the dataset\n",
    "# so we continue with creating the required columns\n",
    "# I made an funtion for this part so its easier to read , and taking noticing that the timestamp is an string i decided to slice the string to get the hour part and then convert it to int to compare it\n",
    "def timeOfDay(hour:int):\n",
    "    if 5 <= hour <= 11:\n",
    "        return \"morning\"\n",
    "    elif 12 <= hour <= 17:\n",
    "        return \"afternoon\"\n",
    "    elif 18 <= hour <= 22:\n",
    "        return \"evening\"\n",
    "    elif hour <0 or hour > 24:\n",
    "        raise Exception(\"invalid hour\")\n",
    "    else:\n",
    "        return \"Night\"\n",
    "\n",
    "RAW_data[\"timeOfDay\"] = RAW_data[\"timestamp\"].apply(lambda x:timeOfDay(int(x[11:-6])))\n",
    "RAW_data[\"year\"] = RAW_data[\"timestamp\"].apply(lambda x:int(x[0:4]))\n",
    "\n",
    "# now we are going to convert the columns and timestamp to their proper datatypes\n",
    "\n",
    "RAW_data[\"timestamp\"] = pd.to_datetime(RAW_data[\"timestamp\"])\n",
    "RAW_data[\"device\"] = RAW_data[\"device\"].astype(\"category\")\n",
    "RAW_data[\"assistant_model\"] = RAW_data[\"assistant_model\"].astype(\"category\")\n",
    "RAW_data[\"timeOfDay\"] = RAW_data[\"timeOfDay\"].astype(\"category\")\n",
    "RAW_data[\"usage_category\"] = RAW_data[\"usage_category\"].astype(\"category\")\n",
    "\n",
    "# note that the numericals stay the same, and I decided to leave year as a number\n",
    "\n",
    "RAW_data\n"
   ],
   "id": "6d77bcfffaf9846e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## *Data Analysis*\n",
    " - Additional Notes etc..."
   ],
   "id": "1e3d7e9c5663dd36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#1. Different AI Assistants used (count and percentage).\n",
    "assistant = RAW_data['assistant_model'] #selecting the column assistant_model\n",
    "count = assistant.value_counts() #counting how many times each assistant appears\n",
    "percentage = ((count/RAW_data['assistant_model'].count())*100).round(2) #calculating the percentage of each assistant occurrence\n",
    "\n",
    "different_assistants = pd.DataFrame({'count': count, 'percentage': percentage}) #making a dataframe with the results\n",
    "print(different_assistants)\n",
    "#There is 5 different AI models in the dataset\n",
    "\n",
    "#2. Average session length per assistant model.\n",
    "\n",
    "average_session_length = RAW_data.groupby('assistant_model')['session_length_minutes'].mean().round(2)\n",
    "average_session_length_output = pd.DataFrame({'Average length': average_session_length})\n",
    "print(average_session_length_output)\n",
    "#Average session length is relatively similar for all AI models, with the highest being GPT-5 at 8.15 and the lowest being o1 at 7.18 minutes\n",
    "\n",
    "#3. Usage category per assistant model\n",
    "usage_per_assistant = pd.pivot_table(RAW_data, index='assistant_model', columns='usage_category', aggfunc='count', values='timestamp')\n",
    "print(usage_per_assistant)\n",
    "# An interesting observation is that o1 is used the most for Writing and Education compared to other categories, most likely due to better reasoning than other models,\n",
    "# In general the most used models are the three GPT models, with GPT-4 being the most consistently used of the three\n",
    "\n",
    "#4. Longest average prompt length and use time per task\n",
    "longest_avg_prompt = RAW_data.groupby('usage_category')['prompt_length'].mean().round(2)\n",
    "print(longest_avg_prompt)\n",
    "\n",
    "longest_avg_time = RAW_data.groupby('usage_category')['session_length_minutes'].mean().round(2)\n",
    "print(longest_avg_time)\n",
    "#The longest average prompt length is in Research with 141.26 characters on average, while the longest average session length is for coding with 8.51 seconds\n",
    "\n",
    "#5. Usage category per time of day\n",
    "usage_category_per_timeOfDay = pd.pivot_table(RAW_data, index='usage_category', columns='timeOfDay', aggfunc='count', values='timestamp')\n",
    "print(usage_category_per_timeOfDay)\n",
    "# Most categories have a specific time of day during which they are the least used. For example, education is least used at night, while writing is the least used during the evening\n",
    "\n",
    "\n",
    "#6. Popularity of assistants over time\n",
    "assistant_model_per_year = pd.pivot_table(RAW_data, index='assistant_model', columns='year', aggfunc='count', values='timestamp')\n",
    "print(assistant_model_per_year)\n",
    "\n",
    "usage_per_year = (RAW_data.groupby(['assistant_model', 'year']).size().reset_index(name='count'))\n",
    "plot.figure(figsize=(10, 7))\n",
    "sea.scatterplot(data=usage_per_year , x='year', y='count', hue='assistant_model')\n",
    "#just one year is given and the most used assistant is GPT-4."
   ],
   "id": "31d91f3c22fcd9d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## code for data analysis here\n",
    "\n",
    "# Visualizations:\n",
    "# – Plot distributions of key features using histograms, KDE plots, and boxplots.\n",
    "# – Use color to distinguish individual assistants.\n",
    "\n",
    "# <Kelvin Maringer>\n",
    "\n",
    "Axis_t = plot.Axes\n",
    "\n",
    "\n",
    "\n",
    "def nicer_dicer(axis: Axis_t,ticks:bool=False) -> None:\n",
    "    \"\"\"Removes underscores from x and y axis labels and tweaks some things because it looks better on the graphs\"\"\"\n",
    "    if not ticks:\n",
    "        axis.set_xlabel(axis.get_xlabel().capitalize().replace(\"_\", \" \").replace(\"minutes\", \"in min.\")) # replaces underscores; capitalizes first letter and formats \"minutes\" to \"in min.\"\n",
    "        axis.set_ylabel(axis.get_ylabel().capitalize().replace(\"_\", \" \").replace(\"minutes\", \"in min.\"))\n",
    "    else:\n",
    "        axis.set_xticklabels([axis_sel.get_text().capitalize().replace(\"_\", \" \").replace(\"minutes\", \"in min.\") for axis_sel in axis.get_xticklabels()]) # same thing, but for tick labels\n",
    "        axis.set_yticklabels([axis_sel.get_text().capitalize().replace(\"_\", \" \").replace(\"minutes\", \"in min.\") for axis_sel in axis.get_yticklabels()])\n",
    "        print(axis.get_xticklabels())\n",
    "\n",
    "\n",
    "def auto_overlay_scaler(axis_bg: Axis_t,axis_fg: Axis_t,plot_bg_x:str, plot_fg_hue:str) -> None:\n",
    "    \"\"\" Automagically scales the front plot to fit inside the background plot\"\"\"\n",
    "    plot_max_val = RAW_data[plot_bg_x].value_counts().astype(float) # get the to-be-plotted values of the background plot\n",
    "\n",
    "\n",
    "    plot_max_y = max(axis_bg.get_ylim()) # gets the maximum y-value of the background plot\n",
    "    for x in plot_max_val.index:\n",
    "        plot_max_val[x] = plot_max_val[x] / plot_max_y # Calculates the percentage height (relative to max y) of each bar in the background plot\n",
    "\n",
    "    #print(plot_max_val,plot_max_y)\n",
    "\n",
    "    max_category_counting = {}\n",
    "\n",
    "    for x in RAW_data[plot_bg_x].unique(): # for each background bar\n",
    "        for name,y in RAW_data[RAW_data[plot_bg_x] == x][plot_fg_hue].value_counts().items(): # for each foreground category inside the background bar\n",
    "            #print(x,y,name)\n",
    "            if x not in max_category_counting.keys():\n",
    "                max_category_counting[x] = y\n",
    "            else:\n",
    "                if y > max_category_counting[x]: # only keep the maximum count of the foreground categories\n",
    "                    max_category_counting[x] = y\n",
    "    #print(max_category_counting)\n",
    "\n",
    "    height_y = 0\n",
    "    for x in max_category_counting.keys(): # for each foreground bars maximum value\n",
    "        if (max_category_counting[x] / plot_max_val[x]) > height_y: # find the overall biggest height percentage that still fits all foreground bars inside the biggest background bar\n",
    "            height_y = (max_category_counting[x] / plot_max_val[x])\n",
    "    axis_fg.set_ylim(0,height_y) # set the y limit of the fg plot to the found maximum\n",
    "\n",
    "    # Note to self: there are better and more clear ways to do this instead of manually counting the data in loops. I am literally shaking rn.\n",
    "\n",
    "#\n",
    "# HISTOGRAMS\n",
    "#\n",
    "\n",
    "combine_plots = True # This toggle will split the histograms from 3 combined plots into 2x3 plots instead. (Better for presentation, worse for comparison imo)\n",
    "\n",
    "fg, axeses = plot.subplots(1 if combine_plots else 2, 3, figsize=(30, 10 if combine_plots else 20)) # Creates a group of 3 plots in a single row (better to look at)\n",
    "fg.suptitle(\"Histograms (∑300 entries)\") # Sum of entries = 300\n",
    "axes = axeses.flatten() if not combine_plots else axeses\n",
    "\n",
    "#sea.kdeplot(data=RAW_data, x=\"satisfaction_rating\", hue=\"assistant_model\", fill=True,alpha=.1,palette=\"muted\")\n",
    "#plot.gca().axes.get_yaxis().set_visible(False) # Hide \"Density\" label since it's annoying\n",
    "\n",
    "\n",
    "\n",
    "# Histograms are chosen to show the distribution amongst assistant models and devices.\n",
    "# This helps to visualize which moedels and devices are actually being used. (NOTE: this sample size is quite small (300 entries) so the data might not be real world applicable)\n",
    "# I cannot imagine that smart speakers are the most used platform for AI assistants, but hey, who knows! maybe i just havn't kept in touch with the latest trends :')\n",
    "# NOTE: who TF uses a smart speaker???\n",
    "\n",
    "sea.histplot(ax=axes[0],data=RAW_data, x=\"assistant_model\",stat=\"count\", palette=\"muted\", hue=\"assistant_model\",legend=False,alpha=.25)\n",
    "axes[0].set_title(\"Assistant Model count\")\n",
    "axes[0].set_ylabel(\"Background plot Count\") # makes it clear that the scale on the left is for the background plot\n",
    "axes[0].set_xlabel(\"\")\n",
    "nicer_dicer(axes[0])\n",
    "print(\"Most people seem to use GPT-4o, which is interesting as it is not exactly the cheapest model available.\")\n",
    "\n",
    "if combine_plots:\n",
    "    axes_0_overlay = axes[0].twinx()\n",
    "    # Dynamic scaling of front plot to stay inside the background plot (looks a lot nicer if the plots dont go outside the background plot)\n",
    "    auto_overlay_scaler(axes[0],axes_0_overlay,\"assistant_model\",\"device\")\n",
    "else: axes_0_overlay = axes[3]\n",
    "\n",
    "sea.countplot(ax=axes_0_overlay,data=RAW_data, x=\"assistant_model\",stat=\"count\", palette=\"muted\", hue=\"device\")\n",
    "axes[0].set_title(\"Assistant Model count\")\n",
    "axes_0_overlay.set_ylabel(\"\")  # Remove y-axis label\n",
    "\n",
    "\n",
    "#  axes[1].tick_params(left=False)  # Remove y-axis ticks\n",
    "\n",
    "print(\"The biggest platform is relatively suprising: smart speakers. I assume that this means devices like Amazon Alexa and Google Home, which is interesting since these devices are not really known for their AI capabilities. Wired... and interesting!\")\n",
    "\n",
    "\n",
    "\n",
    "sea.histplot(ax=axes[1],data=RAW_data, x=\"usage_category\", palette=\"muted\", hue=\"usage_category\",legend=False,alpha=.25)\n",
    "#sea.histplot(ax=axes[2],data=RAW_data, x=\"usage_category\",palette=\"muted\", hue=\"device\",multiple=\"stack\")\n",
    "#sea.histplot(ax=axes[2],data=RAW_data, x=\"device\",palette=\"muted\",alpha=.5, hue=\"usage_category\",multiple=\"stack\")\n",
    "axes[1].set_title(\"Usage Category count\")\n",
    "axes[1].set_ylabel(\"\")  # Remove y-axis label\n",
    "axes[1].set_xlabel(\"\")\n",
    "axes[1].tick_params(axis=\"x\", labelrotation=30) # super Fancy 30° rotation for extra coolness\n",
    "nicer_dicer(axes[1])\n",
    "\n",
    "if combine_plots:\n",
    "    axes_1_overlay = axes[1].twinx()\n",
    "    auto_overlay_scaler(axes[1],axes_1_overlay,\"usage_category\",\"assistant_model\")\n",
    "else: axes_1_overlay = axes[4]\n",
    "\n",
    "\n",
    "\n",
    "sea.countplot(ax=axes_1_overlay, data=RAW_data, x=\"usage_category\", hue=\"assistant_model\", palette=\"muted\")\n",
    "axes_1_overlay.set_ylabel(\"\")  # Remove y-axis label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sea.histplot(ax=axes[2],data=RAW_data, x=\"device\", palette=\"muted\", hue=\"device\",legend=False,alpha=.25)\n",
    "axes[2].set_title(\"Device count\")\n",
    "#sea.move_legend(axes[2], \"upper left\")\n",
    "#  axes[2].set(yticklabels=[])\n",
    "axes[2].set_ylabel(\"\")  # Remove y-axis label\n",
    "axes[2].set_xlabel(\"\") # Remove x-axis label, since it is obvious from context\n",
    "nicer_dicer(axes[2])\n",
    "\n",
    "if combine_plots:\n",
    "    axes_2_overlay = axes[2].twinx()# This combines two plots into one (so both the use counts of devices and the breakdown into usage categories can be seen AT THE SAME TIME.)\n",
    "    auto_overlay_scaler(axes[2],axes_2_overlay,\"device\",\"usage_category\")\n",
    "else: axes_2_overlay = axes[5]\n",
    "\n",
    "sea.countplot(ax=axes_2_overlay, data=RAW_data, x=\"device\", hue=\"usage_category\", palette=\"muted\") # This is so cool. I love it.\n",
    "axes_2_overlay.set_ylabel(\"Foreground plot Count\")  # makes it clear that the scale on the right is for the foreground plot\n",
    "axes_2_overlay.set_xlabel(\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"The most common usage for AI seems to be education. Depending on the definition of this category, this could mean that a lot of people are using these assistats for homework and study-help.\")\n",
    "print(\"This honestly makes me question where this data is from, since I would expect a LOT more people to use AI in a more professional setting (work, coding, writing etc.) rather than for education.\")\n",
    "print(\"Coding one of the least common, which is very suprising to me, considering that AI coding assistants are one of the more ACTUALY USEFUL applications of AI right now. Interesting!\")\n",
    "\n",
    "#desktop_writing\n",
    "#print(RAW_data[(RAW_data[\"device\"]==\"Smart Speaker\") & (RAW_data[\"usage_category\"]==\"Coding\")].value_counts()) #?????????????? -> Why would someone use a smart speaker for coding??? -- This dataset is definetly not real. (Alexa, commit & push the code.)\n",
    "\n",
    "\n",
    "plot.show()\n",
    "\n",
    "\n",
    "#\n",
    "# KDE PLOTS\n",
    "#\n",
    "kdeplots_defcon={\"alpha\":.25,\"fill\":True,\"palette\":\"muted\",\"common_norm\":False} # Default parameters for all KDE plots via **UNPACKING MY BELOVED\n",
    "\n",
    "def cts_kdeplot(ax, data, x, hue,title=None) -> None: # Helper function for creating KDE plots\n",
    "    sea.kdeplot(ax=ax,data=data, x=x, hue=hue, **kdeplots_defcon) # Makes the KDE plot\n",
    "    ax.set_xlim(data[x].min(),data[x].max()) # Set x-axis limits (removes tapering of the KDE at the edges)\n",
    "    ax.set_title(f\"{x} by {hue} type\" if title is None else title) # Title formatting\n",
    "    sea.move_legend(ax, \"lower left\") # self-explanatory\n",
    "    ax.set(yticklabels=[]) # Remove y-axis numbers\n",
    "    ax.set_ylabel(\"\")  # Remove y-axis label\n",
    "    ax.tick_params(left=False)  # Remove y-axis ticks\n",
    "    nicer_dicer(ax)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fg, axes = plot.subplots(1, 3, figsize=(30, 10))\n",
    "fg.suptitle(\"KDE Plots YEAHHH\")\n",
    "\n",
    "cts_kdeplot(axes[0],RAW_data,\"session_length_minutes\",\"device\",\"session length by device type\")\n",
    "\n",
    "#for x in RAW_data[\"device\"].unique():\n",
    "#    print(RAW_data[RAW_data[\"device\"]==x][\"session_length_minutes\"].mean())\n",
    "#    axes[0].axvline(RAW_data[RAW_data[\"device\"]==x][\"session_length_minutes\"].mean(), color='r', linestyle='dashed', linewidth=1, label=\"Mean tokens used\")\n",
    "\n",
    "print(\"This is actually quite interesting! The session length seems to be shorter on desktop devices compared to mobile devices, which is wired. (one would think that desktop users would spend more time because of work/study etc.)\")\n",
    "print(\"Similarly, tablets have the longest session lengths on average. This divide between mobile and desktop could stem from the different use cases for each device type. (or the time it takes to type / enter prompts)\")\n",
    "print(\"This points towards desktop users using the AI assistant for quick queries, while mobile/tablet users might be engaging in longer interactions. Strange!\")\n",
    "\n",
    "cts_kdeplot(axes[1],RAW_data,\"prompt_length\",\"device\",\"prompt length by device type\")\n",
    "\n",
    "print(\"Looking at the prompt lengths, we can see that desktop users tend to have longer prompts on average compared to mobile and tablet users.\")\n",
    "print(\"This could be due to the ease of typing on a physical keyboard, allowing for longer and more complex prompts. This is pretty strange though, since the session lengths were shorter on desktop.\")\n",
    "\n",
    "cts_kdeplot(axes[2],RAW_data,\"tokens_used\",\"assistant_model\",\"tokens used by model type\")\n",
    "\n",
    "print(\"As expected, the more advanced models like GPT-5 and GPT-4o tend to use more tokens on average compared to older models like o1.\")\n",
    "print(\"However, mini seems to use the most tokens on average, which could either stem from its use (longer prompts on average) or its complexity. (which is weired since mini is supposed to be a smaller model). Strange again!\")\n",
    "\n",
    "plot.show()\n",
    "\n",
    "\n",
    "#for x in RAW_data[\"device\"].unique():\n",
    "#    _med = RAW_data[RAW_data[\"device\"]==x][\"session_length_minutes\"].median()\n",
    "#    _mean = RAW_data[RAW_data[\"device\"]==x][\"session_length_minutes\"].mean()\n",
    "#    print(f\"{x} -> median: {_med}, mean: {_mean} :: difference: {_mean - _med}\")\n",
    "# Outliers not significant.\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# BOXPLOTS LEZGO\n",
    "#\n",
    "\n",
    "fg, axes = plot.subplots(1, 2, figsize=(30, 10))\n",
    "fg.suptitle(\"Box Plots\")\n",
    "\n",
    "\n",
    "#sea.boxplot(ax=axes[0],data=RAW_data, x=\"assistant_model\",hue=\"assistant_model\", y=\"satisfaction_rating\", palette=\"muted\",medianprops={\"color\": \"r\", \"linewidth\": 2},notch=True)\n",
    "\n",
    "sea.boxplot(ax=axes[0],data=RAW_data, x=\"assistant_model\",hue=\"assistant_model\", y=\"tokens_used\", palette=\"muted\",medianprops={\"color\": \"r\", \"linewidth\": 2},notch=True)\n",
    "axes[0].set_title(\"Tokens Used by Assistant Model\")\n",
    "nicer_dicer(axes[0])\n",
    "\n",
    "print(\"assistant model vs tokens used is actually quite interesting (genuenly!). One can see that the \\\"advancedness\\\" of the model doesn't really correlate with the number of tokens used. This could either point towards good optimization of the newer models, or simply that users are using the models in different ways.\")\n",
    "print(\"Mini seems to have the highest median token, whilst having a relatively big spread. The token size seems to vary quite a lot.\")\n",
    "\n",
    "print(\"device to session length is also notable, in that the session length seems to be quite stable across the different device types. The spread is also quite small (ranging from ~11 to ~5 minutes on most devices)\")\n",
    "\n",
    "\n",
    "#print(\"Interestingly enough, most models seem to have an completely identical satisfaction rating distribution. This is quite suprising and implies that this data might be synthetic. (i calculated it myself and all models have a mean of exactly 3 with max of .2 difference). Seems like there should be a bigger difference between models, especially considering that each model should (at least in theory) improved over the last.\")\n",
    "#print(\"GPT-5 seems to have the lowest satisfaction, however, the 5.1 version has the highest satisfaction. This could point towards some issues with GPT-5 that were fixed in 5.1. IG\")\n",
    "\n",
    "# <ignore_this>\n",
    "print(RAW_data[[\"assistant_model\",\"satisfaction_rating\"]].groupby(\"assistant_model\").mean())\n",
    "print(RAW_data[[\"assistant_model\",\"satisfaction_rating\"]].groupby(\"assistant_model\").quantile(.75))\n",
    "print(RAW_data[[\"assistant_model\",\"satisfaction_rating\"]].groupby(\"assistant_model\").quantile(.25))\n",
    "# </ignore_this>\n",
    "\n",
    "\n",
    "\n",
    "sea.boxplot(ax=axes[1],data=RAW_data, x=\"device\",hue=\"device\", y=\"session_length_minutes\", palette=\"muted\",medianprops={\"color\": \"r\", \"linewidth\": 2},notch=True)\n",
    "axes[1].set_title(\"Session Length by Device Type\")\n",
    "nicer_dicer(axes[1])\n",
    "plot.show()\n",
    "\n",
    "\n",
    "#  sea.boxplot(data=RAW_data, x=\"assistant_model\",hue=\"assistant_model\", y=\"session_length_minutes\", palette=\"muted\")\n",
    "#  plot.title(\"Boxplot of Session Length by Assistant Model\")\n",
    "#  plot.show()\n",
    "\n",
    "\n"
   ],
   "id": "81a8551d78d7fcaf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Are there any features that clearly differentiate device types?\n",
    "\n",
    "\n",
    "# usage_category seems to be a good indicator as every device has a different, relatively high occurrence in one specific category.\n",
    "# Desktop -> Research\n",
    "# Mobile -> uniquely low coding\n",
    "# Smart Speaker -> Productivity (for whatever reason)\n",
    "# Tablet -> low research\n",
    "# It's not perfect, but this feature could be used to a reasonable degree to differentiate device types. (especially if all sub-categories are taken into account at once)\n",
    "\n",
    "# Session_length_minutes and prompt_length are semi-useful in this case, because there are some slight differences in the distribution\n",
    "# f.e. Desktop users tend to have shorter session lengths on avg. compared to tablet users\n",
    "# And smart speaker users tend to have a very narrow session length distribution (around 8 minutes)\n",
    "\n"
   ],
   "id": "affc737b17362162"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Correction Analysis\n",
    "# - Create a correlation heatmap for all features\n",
    "# - Which features are strongly correlated with each other?\n",
    "# - Are any features highly correlated with assistant models and session length?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "RAW_copy = RAW_data.copy(deep=True).drop(columns=[\"year\",\"timeOfDay\"])\n",
    "\n",
    "for col in RAW_copy.select_dtypes(include=[\"category\"]).columns:\n",
    "    RAW_copy[col] = RAW_copy[col].cat.codes # Convert categories to numerical codes to correlate them (the assignment specifically says to correlate ALL features, so this is necessary)\n",
    "\n",
    "\n",
    "for col in RAW_copy.select_dtypes(include=np.number).columns: # apperently this is redundant.\n",
    "    RAW_copy[col] = (RAW_copy[col] - RAW_copy[col].min()) / (RAW_copy[col].max() - RAW_copy[col].min()) # Normalization\n",
    "\n",
    "print(RAW_copy.head())\n",
    "\n",
    "corr = RAW_copy[RAW_copy.columns.difference([\"year\"])].corr(numeric_only=True)\n",
    "#print(corr)\n",
    "\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))# This cuts the heatmap in half, since the correlation matrix is symmetrical - Nice!\n",
    "\n",
    "axis = sea.heatmap(corr, annot=True, fmt=\".2f\",mask=mask, cmap=\"viridis\")\n",
    "axis.set_title(\"Correlation Heatmap of Features\")\n",
    "nicer_dicer(axis,ticks=True)\n",
    "axis.set_xticks(axis.get_xticks()-1)\n",
    "axis.set_yticks(axis.get_yticks()+1) # This removes the (after trianle masking) no longer needed ticks / labels in the empty spaces\n",
    "#axis.tick_params(axis=\"x\", labelrotation=60) # super Fancy 45° rotation for extra coolness #Number 2 Edit: Nvm, looks like ---\n",
    "#axis.set_xticklabels([x.get_text() if i == 0 else \"\" for i, x in enumerate(axis.get_xticklabels())])\n",
    "\n",
    "plot.show()\n",
    "\n",
    "print(\"There are no really strong correlations between ANY features. The strongest correlations are between [satisfaction:rating, prompt_length] and [usage_category, prompt_length]. both hover arround .1 which is quite weak.\")\n",
    "print(\"This is relatively suprising, as one would imagine that at least some features would have some correlation (like tokens_used and prompt_length). This is another indicator that this dataset might be synthetic. (like the coding on smart speakers...)\")\n",
    "\n",
    "print(\"the most associated feature with assistant_model is usage_category at .06. This is, again, quite weak. It makes sense though, other than the fact that it is the strongest correlation.\")\n",
    "\n",
    "print(\"The most associated feature with session_length_minutes is tokens_used at .07. Again, no real correlation. This one really doesn't make sens though.\")\n",
    "\n",
    "\n"
   ],
   "id": "37fbbd2140548ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Use pair plots to explore feature relationships, color-coded by the assistant model\n",
    "\n",
    "#axis = sea.pairplot(RAW_copy[RAW_copy.columns.difference([\"year\",\"timeOfDay\"])] , hue=\"assistant_model\", palette=\"muted\", diag_kind=\"kde\")\n",
    "\n",
    "axis = sea.pairplot(RAW_data[RAW_data.columns.difference([\"year\",\"timeOfDay\",\"satisfaction_rating\"])] , hue=\"assistant_model\", palette=\"muted\")\n",
    "\n",
    "for ax in axis.axes.flatten():\n",
    "    nicer_dicer(ax)\n",
    "plot.suptitle(\"Pair Plot of Features by Assistant Model\",y =1) # for some reason the title overlaps with the plots without y=1\n",
    "plot.show()\n",
    "print(\"To be quite honest, i don't really know what to make of this. There are no clear patterns or relationships that stand out. This alignes pretty well with the correlation heatmap from before and with the assumption that this dataset might possibly maybe be synthetic.\")\n",
    "print(\"The correlations look like random noise, with no real patterns or relationships that can be found. Not even a little bit of a trend.\")"
   ],
   "id": "ae76a29cffd1cb48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Grouped by assistant model, calculate mean, median and standard deviation for each feature.\n",
    "# - What patterns do you observe?\n",
    "\n",
    "#detailed_stats = RAW_data.select_dtypes(include=[\"number\"]).groupby(\"assistant_model\")\n",
    "detailed_stats = RAW_data.select_dtypes(include=[\"number\"]) # Get all numeric features\n",
    "detailed_stats.drop(columns=[\"year\"],inplace=True)# drop year for stats\n",
    "detailed_stats = detailed_stats.assign(assistant_model=RAW_data[\"assistant_model\"])# add assistant_model back for grouping\n",
    "detailed_stats = detailed_stats.groupby(\"assistant_model\")# grouping\n",
    "\n",
    "print(\"MEAN VALUES ::\")\n",
    "print(detailed_stats.mean())\n",
    "\n",
    "print(\"All means are relatively close to each other across all models with only a few exceptions.\")\n",
    "print(\"o1 has a notably low mean session_length_minutes (7.18) compared to other models which hover around 7.5-8.1 minutes.\")\n",
    "print(\"Mini has a notably high mean tokens_used (882) which is way above other models which are all arround 740-765. This effect can also be observed in the boxplot from before.\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\\nMEDIAN VALUES ::\")\n",
    "print(detailed_stats.median())\n",
    "\n",
    "print(\"the medians are also relatively close to each other (in some cases, aka satisfaction_rating, they are identical).\")\n",
    "print(\"The only real exception is in tokens_used where the spread is quite high. o1, GPT-4o and GPT-5 are relatively low (arround 730-775), whilst GPT-5.1 is quite high (805) and mini is, again, very high at a whopping 985 tokens.\")\n",
    "\n",
    "print(\"\\n\\nSTANDARD DEV. VALUES ::\")\n",
    "print(detailed_stats.std())\n",
    "\n",
    "print(\"The standard deviations differ more between models compared to the means and medians.\")\n",
    "print(\"prompt_length has a notably low std.dev for Mini (58.5) compared to other models which are all arround 70-75.\")\n",
    "print(\"\\n\\n The conclusion I draw from this is that the models are quite similar in most cases, with 01 and Mini being the most notable outliers.\")\n",
    "\n"
   ],
   "id": "ac8a0aa9e38ed49"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Time-Based Analysis:\n",
    "#– How does session length vary by day or time of day?\n",
    "#– Are there peak usage times for certain assistants?\n",
    "#– Create a line plot or a heatmap to show usage trends over time.\n",
    "\n",
    "# Allrighty then, L E T S  G O\n",
    "# NOTE: I am coding this on my alexa, github enabled, smart speaker, so please excuse any typos in my code!\n",
    "#⣿⣿⣿⣿⣿⣿⣿⣿⡿⠿⠛⠛⠛⠋⠉⠈⠉⠉⠉⠉⠛⠻⢿⣿⣿⣿⣿⣿⣿⣿⠀\n",
    "#⣿⣿⣿⣿⣿⡿⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠛⢿⣿⣿⣿⣿⠀\n",
    "#⣿⣿⣿⣿⡏⣀⠀⠀⠀⠀⠀⠀⠀⣀⣤⣤⣤⣄⡀⠀⠀⠀⠀⠀⠀⠀⠙⢿⣿⣿⠀\n",
    "#⣿⣿⣿⢏⣴⣿⣷⠀⠀⠀⠀⠀⢾⣿⣿⣿⣿⣿⣿⡆⠀⠀⠀⠀⠀⠀⠀⠈⣿⣿⠀\n",
    "#⣿⣿⣟⣾⣿⡟⠁⠀⠀⠀⠀⠀⢀⣾⣿⣿⣿⣿⣿⣷⢢⠀⠀⠀⠀⠀⠀⠀⢸⣿⠀\n",
    "#⣿⣿⣿⣿⣟⠀⡴⠄⠀⠀⠀⠀⠀⠀⠙⠻⣿⣿⣿⣿⣷⣄⠀⠀⠀⠀⠀⠀⠀⣿⠀\n",
    "#⣿⣿⣿⠟⠻⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠶⢴⣿⣿⣿⣿⣿⣧⠀⠀⠀⠀⠀⠀⣿⠀\n",
    "#⣿⣁⡀⠀⠀⢰⢠⣦⠀⠀⠀⠀⠀⠀⠀⠀⢀⣼⣿⣿⣿⣿⣿⡄⠀⣴⣶⣿⡄⣿⠀\n",
    "#⣿⡋⠀⠀⠀⠎⢸⣿⡆⠀⠀⠀⠀⠀⠀⣴⣿⣿⣿⣿⣿⣿⣿⠗⢘⣿⣟⠛⠿⣼⠀\n",
    "#⣿⣿⠋⢀⡌⢰⣿⡿⢿⡀⠀⠀⠀⠀⠀⠙⠿⣿⣿⣿⣿⣿⡇⠀⢸⣿⣿⣧⢀⣼⠀\n",
    "#⣿⣿⣷⢻⠄⠘⠛⠋⠛⠃⠀⠀⠀⠀⠀⢿⣧⠈⠉⠙⠛⠋⠀⠀⠀⣿⣿⣿⣿⣿⠀\n",
    "#⣿⣿⣧⠀⠈⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠟⠀⠀⠀⠀⢀⢃⠀⠀⢸⣿⣿⣿⣿⠀\n",
    "#⣿⣿⡿⠀⠴⢗⣠⣤⣴⡶⠶⠖⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡸⠀⣿⣿⣿⣿⠀\n",
    "#⣿⣿⣿⡀⢠⣾⣿⠏⠀⠠⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⠉⠀⣿⣿⣿⣿⠀\n",
    "#⣿⣿⣿⣧⠈⢹⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⣿⣿⣿⣿⠀\n",
    "#⣿⣿⣿⣿⡄⠈⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣠⣴⣾⣿⣿⣿⣿⣿⠀\n",
    "#⣿⣿⣿⣿⣧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣠⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀\n",
    "#⣿⣿⣿⣿⣷⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀\n",
    "#⣿⣿⣿⣿⣿⣦⣄⣀⣀⣀⣀⠀⠀⠀⠀⠘⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀\n",
    "#⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⡄⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀\n",
    "#⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣧⠀⠀⠀⠙⣿⣿⡟⢻⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀\n",
    "#⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠇⠀⠁⠀⠀⠹⣿⠃⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀\n",
    "#⣿⣿⣿⣿⣿⣿⣿⣿⡿⠛⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⢐⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀\n",
    "#⣿⣿⣿⣿⠿⠛⠉⠉⠁⠀⢻⣿⡇⠀⠀⠀⠀⠀⠀⢀⠈⣿⣿⡿⠉⠛⠛⠛⠉⠉⠀\n",
    "#⣿⡿⠋⠁⠀⠀⢀⣀⣠⡴⣸⣿⣇⡄⠀⠀⠀⠀⢀⡿⠄⠙⠛⠀⣀⣠⣤⣤⠄⠀⠀\n",
    "\n",
    "\n",
    "RAW_data[\"hour\"] = RAW_data[\"timestamp\"].dt.hour\n",
    "RAW_data[\"dayofweek\"] = RAW_data[\"timestamp\"].dt.dayofweek\n",
    "daylist = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "RAW_data[\"dayofweek_name\"] = RAW_data[\"dayofweek\"].apply(lambda x: daylist[x])\n",
    "\n",
    "timeofday_def = {(5,11):\"Morning\", (11,17):\"Midday\", (17,23):\"Evening\", (23,5):\"Night\"}\n",
    "def _map_timeofday_def(hour):\n",
    "    for x in timeofday_def.keys():\n",
    "        if hour >= x[0] and hour < x[1]:\n",
    "            return timeofday_def[x]\n",
    "        elif x[0] > x[1]: # for the night range which wraps around midnight\n",
    "            if hour >= x[0] or hour < x[1]:\n",
    "                return timeofday_def[x]\n",
    "    raise ValueError(f\"Hour not in any defined range {hour}\") #should never happen\n",
    "\n",
    "RAW_data[\"timeofday\"] = RAW_data[\"hour\"].apply(_map_timeofday_def) # had to google what the translation for \"mittag\" is. is midday even a word???\n",
    "#print(RAW_data[[\"timestamp\",\"hour\",\"dayofweek\",\"timeofday\"]].head(10))\n",
    "\n",
    "\n",
    "\n",
    "fg, axes = plot.subplots(1, 2, figsize=(30, 10))\n",
    "fg.suptitle(\"Time-Based Analysis via Boxplots\")\n",
    "\n",
    "sea.boxplot(ax=axes[0],data=RAW_data, x=\"timeofday\",hue=\"timeofday\", y=\"session_length_minutes\", palette=\"muted\",medianprops={\"color\": \"r\", \"linewidth\": 2},notch=True,order=timeofday_def.values())\n",
    "axes[0].set_title(\"Session Length by Time of Day\")\n",
    "nicer_dicer(axes[0])\n",
    "print(\"session_length_minutes seems to be quite stable across the day. There is a slight trend towards longer sessions the later it gets, but it's quite small\")\n",
    "\n",
    "\n",
    "sea.boxplot(ax=axes[1],data=RAW_data, x=\"dayofweek_name\",hue=\"dayofweek_name\", y=\"session_length_minutes\", palette=\"muted\",medianprops={\"color\": \"r\", \"linewidth\": 2},notch=True,order = daylist)\n",
    "axes[1].set_title(\"Session Length by Day of Week\")\n",
    "nicer_dicer(axes[1])\n",
    "print(\"The day of the week however varies quite a lot, which is suprising (and a welcome change from the rest of the dataset). Especially monday seems to have notacibly longer sessions compared to other days. the spread is also quite high across the board.\")\n",
    "plot.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fg, axes = plot.subplots(1, 2, figsize=(30, 10))\n",
    "fg.suptitle(\"Time-Based Analysis via Histograms\")\n",
    "\n",
    "cts_kdeplot(axes[0],RAW_data,\"hour\",\"assistant_model\",\"session hour by assistant model over the day\")\n",
    "print(\"There seems to be a noticable falloff in the usage of Mini during later hours, whilst other models remain relatively stable. The general trend seems to be a slight decrease the later it gets, which makes sense.\")\n",
    "print(\"the only exception to this rule is GPT-5 which slightly increases in usage during the night\")\n",
    "print(\"Overall, the usage seems to have noticable peaks for each model (excl. o1 and GPT-5.1).\")\n",
    "\n",
    "\n",
    "\n",
    "cts_kdeplot(axes[1],RAW_data,\"session_length_minutes\",\"dayofweek_name\",\"session length by day of week\")\n",
    "\n",
    "plot.show()\n",
    "\n",
    "\n",
    "\n",
    "fg, axes = plot.subplots(1, 2, figsize=(30, 10))\n",
    "fg.suptitle(\"Time-Based Analysis via Boxplots\")\n",
    "\n",
    "hourly_session_length = (RAW_data.groupby(\"hour\")[\"session_length_minutes\"].mean().reset_index()) # calculate mean session_length per hour\n",
    "\n",
    "\n",
    "\n",
    "sea.lineplot(ax=axes[0],data=hourly_session_length, x=\"hour\", y=\"session_length_minutes\", palette=\"muted\")\n",
    "axes[0].set_title(\"Session Length by Time of Day\")\n",
    "nicer_dicer(axes[0])\n",
    "\n",
    "\n",
    "sea.lineplot(ax=axes[1],data=RAW_data, x=\"timeofday\",hue=\"assistant_model\", y=\"session_length_minutes\", palette=\"muted\")\n",
    "axes[1].set_title(\"Session Length by Day of Week\")\n",
    "nicer_dicer(axes[1])\n",
    "\n",
    "plot.show()\n",
    "\n",
    "\n",
    "corr = RAW_data.pivot(index=\"timestamp\", columns=\"usage_category\", values=\"hour\")\n",
    "\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))# This cuts the heatmap in half, since the correlation matrix is symmetrical - Nice!\n",
    "\n",
    "axis = sea.heatmap(corr, annot=True, fmt=\".2f\",mask=mask, cmap=\"viridis\")\n",
    "axis.set_title(\"Correlation Heatmap of Features\")\n",
    "nicer_dicer(axis,ticks=True)\n",
    "axis.set_xticks(axis.get_xticks()-1)\n",
    "axis.set_yticks(axis.get_yticks()+1) # This removes the (after trianle masking) no longer needed ticks / labels in the empty spaces\n",
    "\n",
    "plot.show()\n",
    "\n",
    "# </Kelvin Maringer>"
   ],
   "id": "d82db29054a20e15"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Additional notes for data Analysis:\n",
    "There is a marker for synthetic datasets on kaggle (This dataset was synthetically generated and may not reflect real-world data.)\n",
    "Therefore there are 3 options for this data:\n",
    " 1. It's synthetic still\n",
    " Maybe the author doesn't care and tries to pass this of as real world data (possible)\n",
    " 2. It's real, but doesn't reflect reality at all / not enough data to analyze properly:\n",
    " Maybe this is just a very specific tiny slice in the AI Assistant usage, which would explain why the data is so wired.\n",
    " Also: This is not a lot of data to work with. 300 entries are not substantial enough for actual pattern recognition / analysis (IN THIS SPECIFIC CASE)\n",
    " 3. It's real and actually reflects realyity:\n",
    " People that use smart speakers for coding scare me."
   ],
   "id": "ec5e22e49391e113"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Additional Insights\n",
    " - Additional Notes etc..."
   ],
   "id": "59837978e2921ec4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#1 custom Analysis one The time period of this dataset is 3 months 10 days or 69 days\n",
    "time_period = RAW_data['timestamp'].max() - RAW_data['timestamp'].min()\n",
    "print(f\"The time period of this dataset is {time_period.days} days\")\n",
    "# the time period of this dataset is only 69 days\n",
    "\n",
    "#2 custom Analysis two The tokens used per minute by assistant model\n",
    "Tokens_per_minute = RAW_data[[\"assistant_model\",\"session_length_minutes\",\"tokens_used\"]]\n",
    "Tokens_per_minute[\"tokens_per_minute\"] = Tokens_per_minute[\"tokens_used\"]/Tokens_per_minute[\"session_length_minutes\"]\n",
    "plot.figure(figsize=(10,6))\n",
    "sea.barplot(Tokens_per_minute, x='assistant_model', y='tokens_per_minute', palette=\"muted\")\n",
    "\n",
    "\n",
    "#from here we can see that 01 is the one with the highest satisfaction rating 3.1 while gpt-4 has the lowest with 2.96\n",
    "\n",
    "#3 custom Analysis three The relation btw device and satisfaction rating\n",
    "Complexity = RAW_data[[\"satisfaction_rating\",\"device\"]]\n",
    "plot.figure(figsize=(10,6))\n",
    "sea.barplot(Complexity,x='device', y='satisfaction_rating', palette=\"muted\")\n",
    "\n",
    "# here we analyze the relation between device and usage category , we can see that for any reason speaker is the most used device for productivity tasks and coding Btw others\n",
    "#4 custom Analysis - the relationship betweeen weekday and time of day\n",
    "time_data = RAW_data.copy()\n",
    "\n",
    "time_data['weekday'] = time_data['timestamp'].dt.day_name()\n",
    "grouped = time_data.groupby(['weekday', 'timeOfDay']).size().unstack(fill_value=0)\n",
    "week_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "grouped = grouped.reindex(week_order)\n",
    "plot.figure(figsize=(10,8))\n",
    "sea.heatmap(grouped, annot=True)\n",
    "plot.show()\n",
    "\n",
    "\n",
    "#5 custom Analysis - Average session length per usage category\n",
    "avg_session_by_task = RAW_data.groupby('usage_category', as_index=False)['session_length_minutes'].mean()\n",
    "avg_session_by_task = avg_session_by_task.sort_values('session_length_minutes',ascending=False)\n",
    "print(\"Average session length by task type:\\n\", avg_session_by_task)\n",
    "plot.figure(figsize=(10,6))\n",
    "sea.barplot(data=avg_session_by_task, x='usage_category', y='session_length_minutes')\n",
    "plot.show()\n"
   ],
   "id": "786ea580222b59d4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OPY_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
